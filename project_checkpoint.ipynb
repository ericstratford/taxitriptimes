{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db96afeb",
   "metadata": {},
   "source": [
    "# ERIC STRATFORD\n",
    "## CSE 151B Project\n",
    "### Checkpoint Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fdf9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor \n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def distance(p1, p2):\n",
    "    if isinstance(p1[0], str):\n",
    "        return np.sqrt((float(p2[0])-float(p1[0]))**2+(float(p2[1])-float(p1[1])**2))\n",
    "    else:\n",
    "        return np.sqrt((p2[0]-p1[0])**2+(p2[1]-p1[1])**2)\n",
    "\n",
    "def normalize_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data\n",
    "\n",
    "def convert_string_to_list(string):\n",
    "    return eval(string)\n",
    "\n",
    "def transformdf(df):\n",
    "    data = df.copy()\n",
    "    data = data[data['MISSING_DATA']==False]\n",
    "    if 'POLYLINE' in data.columns:\n",
    "        data[\"Count\"] = data['POLYLINE'].apply(lambda x: max(x.count(\"[\") - 1, 0))\n",
    "        data[\"Travel Time (s)\"] = data['Count']*15\n",
    "        data[\"Travel Time (m)\"] = data['Travel Time (s)']/60\n",
    "    data['Time'] = pd.to_datetime(data['TIMESTAMP'], unit='s')\n",
    "    data['Year'] = data['Time'].dt.year\n",
    "    data['Month'] = data['Time'].dt.month\n",
    "    data['Day of Month'] = data['Time'].dt.day\n",
    "    data['Day of Week'] = data['Time'].dt.day_name()\n",
    "    data['Hour'] = data['Time'].dt.hour\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogdata = pd.read_csv(\"train.csv\")\n",
    "ogdata.head(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transformdf(ogdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24bce4",
   "metadata": {},
   "source": [
    "## Deep Learning Model and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ed942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature set and test/training split\n",
    "features = data[['CALL_TYPE', 'ORIGIN_STAND', 'TAXI_ID', 'DAY_TYPE', 'Year', 'Month', 'Day of Month', 'Day of Week', 'Hour']]\n",
    "target = data['Travel Time (s)']\n",
    "test_size = 0.2\n",
    "seed = 69\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70678702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform and preprocess\n",
    "\n",
    "cat_features = ['Hour', 'Day of Week', 'Month', 'TAXI_ID', 'Day of Month', 'Year', 'CALL_TYPE']\n",
    "\n",
    "# Preprocess with one-hot-encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ])\n",
    "\n",
    "px_train = preprocessor.fit_transform(x_train)\n",
    "\n",
    "# Transform the test data\n",
    "px_test = preprocessor.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fcc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the MLP model\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(5, 5)\n",
    "                    , activation='relu'\n",
    "                    , solver='adam'\n",
    "                    , random_state=69\n",
    "                    , max_iter=100\n",
    "                    , batch_size=128\n",
    "                    , learning_rate='constant'\n",
    "                    , learning_rate_init=0.001\n",
    "                    , momentum=0.9\n",
    "                  )\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit model\n",
    "mlp.fit(px_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(px_test)\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Get Total Training Time\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training Time:\", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a195d",
   "metadata": {},
   "source": [
    "## LOSS COMPARISON\n",
    "### Section 3.A.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759b629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOSS COMPARISON CELL\n",
    "# DO NOT RUN EXCEPT FOR LOSS COMPARISON\n",
    "\n",
    "# Initialize loss dataframe for comparison\n",
    "loss_df = pd.DataFrame({\"Epoch\":range(1,21)})\n",
    "\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    \n",
    "    # Create and train the MLP model\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(5, 5)\n",
    "                        , activation='relu'\n",
    "                        , solver='adam'\n",
    "                        , random_state=42\n",
    "                        , max_iter=20\n",
    "                        , batch_size=128\n",
    "                        , learning_rate='constant'\n",
    "                        , learning_rate_init=learning_rate\n",
    "                        , momentum=0.9\n",
    "                      )\n",
    "\n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    mlp.fit(px_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = mlp.predict(px_test)\n",
    "\n",
    "    \n",
    "    # Add loss to dataframes\n",
    "    col_name = str(learning_rate)\n",
    "    loss_df[col_name] = mlp.loss_curve_\n",
    "    loss_df[col_name] = loss_df[col_name].apply(np.sqrt)\n",
    "    \n",
    "    \n",
    "    # Print learning rate\n",
    "    print(\"Learning Rate:\", learning_rate)\n",
    "    \n",
    "    # Calculate the root mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(\"RMSE:\", rmse)\n",
    "    \n",
    "    # Get Total Training Time\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"Training Time:\", training_time)\n",
    "    \n",
    "    \n",
    "# Graph the loss over each iteration for each learning rate\n",
    "loss_df.plot(x='Epoch', y=['0.1', '0.01', '0.001', '0.0001', '1e-05', '1e-06'], figsize = (12,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085053c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'mlp2_model.sav'\n",
    "pickle.dump(mlp, open(filename, 'wb')) \n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# Best validation RMSE: 735.57\n",
    "# Corresponding test RMSE: 657.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a71209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on kaggle validation set\n",
    "\n",
    "# Import validation set\n",
    "validation_orig = pd.read_csv(\"test_public.csv\")\n",
    "validation = transformdf(validation_orig)\n",
    "\n",
    "# Reconfigure preprocessing with same method as training\n",
    "cat_features = ['Hour', 'Day of Week', 'Month', 'TAXI_ID', 'Day of Month', 'Year', 'CALL_TYPE']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ])\n",
    "px_train = preprocessor.fit_transform(x_train)\n",
    "\n",
    "# Run model on validation set\n",
    "val_features = validation[['CALL_TYPE', 'ORIGIN_STAND', 'TAXI_ID', 'DAY_TYPE', 'Year', 'Month', 'Day of Month', 'Day of Week', 'Hour']]\n",
    "val_set = preprocessor.transform(val_features)\n",
    "predictions = mlp.predict(val_set)\n",
    "\n",
    "# Format validation predictions output\n",
    "validation[\"TRAVEL_TIME\"] = predictions\n",
    "val_out = validation[['TRIP_ID', 'TRAVEL_TIME']]\n",
    "val_out['TRIP_ID'] = val_out['TRIP_ID'].astype(str)\n",
    "\n",
    "# Export validation predictions\n",
    "val_csv = val_out.to_csv('val_pred.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
